# local.ai

A desktop app for hosting an inference API on your local machine. Binary distribution will be released soon once Code Signing is set up. If you're experienced in this matter, feel free to reach out!

It's made to be used alongside https://github.com/alexanderatallah/window.ai/ as a simple way to have a local inference server up and running with just the model files.

## Demo

- TBD

## Development

Here's how to run the project locally:

### Prerequisites

1. node >= 18
2. rust >= 1.69
3. pnpm >= 8

### Workflow

```
pnpm i
pnpm dev:desktop
```

## Future:

- Auto update server
- LLM model downloader
- Start as many inference endpoints/ports as needed
- (NTH): Automated release bundling

> NTH: Nice to have
